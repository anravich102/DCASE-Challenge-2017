import numpy as np 
import os,sys
from PARAMS import *
import datetime


def save_raw_predictions(predictions, model, directory, featureParams, modelParams,
                         event, numFramesList, fileList):
    rawFile = directory+event+ '_raw_Predictions.h5'

    d = { 'predictions' : predictions,
           'model':model,
           'directory':directory,
           'featureParams':featureParams,
           'modelParams' :modelParams,
           'event':event,
           'numFramesList':numFramesList,
           'fileList':fileList


    }

    with h5py.File(rawFile,'w') as f:
        f.create_dataset('d',data = d)

    return rawFile

import h5py

def save_processed_predictions(pp, model, directory, featureParams, modelParams,
                         event, numFramesList, fileList):         #raw predictions for now
    processedFile = directory+event+ '_processed_predictions.h5'

    d = { 'pp' : pp,
           'model':model,
           'directory':directory,
           'featureParams':featureParams,
           'modelParams' :modelParams,
           'event':event,
           'numFramesList':numFramesList,
           'fileList':fileList


    }


    with h5py.File(processedFile, 'w') as h5f:
        h5f.create_dataset('pp', data=pp)

    print("Processed predictions saved to: ", processedFile)

    return processedFile


def ensemble_similar_predictions(ListofFiles, type = 'average_before'):

    temp = []
    for name in ListofFiles:
        with h5py.File(name, 'r') as f:
            temp.append(f['pp'][:])
    
    
    return sum(temp)/len(temp)

    


def write_raw_predictions( predictions, model, directory, featureParams, modelParams,
                         event, numFramesList, filesList):
    rawPredictionsFile = model+directory+ '_rawPredictions.txt'
    index = 0
    count = 0
    with open(rawPredictionsFile, 'w') as ff:
        for file,numFramesInFile in zip(filesList,numFramesList):
            

            if(count>100):
                continue
            count+=1
        #files.append(file)
            predictionsForFile = predictions[index:index+numFramesInFile]
            #print('shape of predictions for file', predictionsForFile.shape)
            #print(file, predictionsForFile)
            index = index + numFramesInFile

            ff.write(file + '\n' )
            ff.write( str(predictionsForFile.tolist()) )
            ff.write( '\n' )

    print('wrote raw predictions (for 100 files) to :' , os.path.join(os.getcwd(), rawPredictionsFile))


def process_predictions(predictions, model, directory, featureParams, modelParams,
                         event, numFramesList, filesList, mode = None):

    '''numFrames is the number of samples/images generated by a single audio file '''
    ''' directory is the model + timestamp string '''
    
    '''numFramesList,filesList must be passed from while loading test/evaluation data'''

   

    # write_raw_predictions( predictions, model, directory, featureParams, modelParams,
    #                      event, numFramesList, filesList)


    files = []
    startTimes = []
    endTimes = []

    # print("writing sample start times to file")
    # num_samples = 50
    # get_sample_start_times(predictions, filesList[:num_samples],numFramesList[:num_samples],
    #                          model,directory,featureParams
    #                          , modelParams, event)
    # print("done writing sample start times! ")

    index = 0
    for file,numFramesInFile in zip(filesList,numFramesList):
        
        #files.append(file)
        predictionsForFile = predictions[index:index+numFramesInFile]
        index = index + numFramesInFile


        startTime,endTime = get_times(predictionsForFile,file,numFramesInFile, event, model,
                                     modelParams, featureParams)

        startTimes.append(startTime)
        endTimes.append(endTime)

    annotatedFile = write_output_file(filesList, startTimes, endTimes, directory,
                                       event, modelParams,featureParams, mode = mode)

    return annotatedFile




def get_times(predictionsForFile,file,numFramesInFile, event, model,
                                     modelParams, featureParams):
    
    classify_threshold = modelParams[model][event]['classify_threshold']
    # pass predictionForFile as an array 
    kernel_size = modelParams[model][event]['medfilt_kernel']
    processedPredictions = post_process(predictionsForFile, classify_threshold,kernel_size)

    pp = processedPredictions

    #pp is a bunch of zeros or ones only, with a sungle chunk of ones.

    if model=='dnn' or model == 'rnn':
        if np.array_equal(pp, np.zeros(pp.shape)):
            startTime = -1.0
            endTime = -1.0
        else:
            y = np.where(pp == 1.0)



            index = y[0][0]
            endIndex = y[0][-1]
            startTime = index * ((100.0 - featureParams[model][event]['percent_overlap'])/100.0)*\
                            featureParams[model][event]['win_length']

            endTime = endIndex * ((100.0 - featureParams[model][event]['percent_overlap'])/100.0)*\
                            featureParams[model][event]['win_length']

    if model=='cnn':
        if np.array_equal(pp, np.zeros(pp.shape)):
            startTime = -1.0
            endTime = -1.0
        else:
            y = np.where(pp == 1.0)

            index = y[0][0]
            endIndex = y[0][-1]
            startTime = index * ((100.0 - featureParams[model][event]['percent_overlap'])/100.0)*\
                            featureParams[model][event]['win_length']

            endTime = endIndex * ((100.0 - featureParams[model][event]['percent_overlap'])/100.0)*\
                            featureParams[model][event]['win_length']


 
    
    return startTime, endTime

import scipy.signal
def post_process(predictionsForFile, classify_threshold,kernel_size):
    # filtered = scipy.signal.medfilt(predictionsForFile, kernel_size=3)
    # filtered = filtered.tolist()
    # pp = [round(x[0]) for x in filtered]
    # return np.asarray(pp)
    modelParams = loadfeatureParams('modelParams.yaml')

    classify = [1.0 if x > classify_threshold else 0.0 for x in predictionsForFile ]

    return scipy.signal.medfilt(classify,kernel_size = kernel_size)



def get_sample_start_times(predictions, filesList,numFramesList,model,
                            directory,featureParams, modelParams, event):
    '''print ALL the start times of 20 files just to see what is happening'''
    index = 0
    i = 0           
    allStartTimes = [None]*len(filesList)
    for file,numFramesInFile in zip(filesList,numFramesList):
        
        #files.append(file) 
        predictionsForFile = predictions[index:index+numFramesInFile]
        index = index + numFramesInFile

        # startTime,endTime = get_times(predictionsForFile,file,numFramesInFile, event, model,
        #                              modelParams, featureParams)

        # startTimes.append(startTime)
        # endTimes.append(endTime)
        classify_threshold = modelParams[model][event]['classify_threshold']
        
        kernel_size = modelParams[model][event]['medfilt_kernel']
        pp =  post_process(predictionsForFile,classify_threshold,kernel_size)
        

        if model=='dnn' or model == 'cnn' or model == 'rnn':
            if np.array_equal(pp, np.zeros(pp.shape)):
                startTime = -1.0
                endTime = -1.0
                allStartTimes[i] = ['All zero. No event frame detected.']
            else:
                y = np.where(pp == 1.0)


                tr = ((100.0 - featureParams[model][event]['percent_overlap'])/100.0)*\
                                featureParams[model][event]['win_length']
                
                array = y[0]
                l = []
                for item in array:
                    l.append(float(item*tr)) 
                allStartTimes[i] = l


                
        i = i+1                        

    filename = model+directory+'_samplesFile' +'.txt'
    with open(filename,'w') as f:  
        for file,numFramesInFile,thelist in zip(filesList,numFramesList,allStartTimes):
            #print(len(allStartTimes))
            #print(thelist)
            f.write(file + '\t') 
            for item in thelist:
                f.write("%s\t" % item)
            f.write("\n")

    print('wrote all start times to :', filename )

def write_output_file(files, startTimes, endTimes , directory ,event, 
                        modelParams, featureParams, mode = None):

    '''start time contains -1 if event did not occur'''
    '''end time contains final window timestamp if event continues till end'''
    

    #first make the required directories:
    resultsDir = os.path.join(os.getcwd(), '../Results')
    if not os.path.exists(resultsDir):
        os.makedirs(resultsDir)
    

    annotatedFile = directory + '_' + event + '_' + mode + '_' + datetime.datetime.now().strftime("%I_%M_%p_%B_%d_%Y") + '.csv'

    pathToSave = os.path.join(resultsDir,annotatedFile)

    #length = len(files)
   
    length = 500

    mainList = [0]*length
    #print(len(files))
    #print(len(startTimes))
    #print(len(endTimes))
    for file, start, end in zip(files, startTimes,endTimes):
        index =  int(file.split("_")[3])
        #print (index)
        
        mainList[index] = [file, start,end]

    # for i in range(500):
    #     if mainList[i] == None:
    #         print(i)
    

    with open(pathToSave,'w') as f:
        for item in mainList:
            if item == 0:
                continue
            file = item[0]
            start = float(item[1])
            end = float(item[2])
            if(start != -1.0):
                f.write(file + "\t" + str(round(start, 4)) + "\t" + str(round(end, 4)) + "\t" + event + "\n")
            else:
                f.write(file +  "\n")




    # save parameters  of the currrent model (using same name as  directory name)
    # for reference later:
    file1 = directory+'modelParams.yaml'
    pathToFile1 = os.path.join(resultsDir, file1)

    file2 = directory+'featureParams.yaml'
    pathToFile2 = os.path.join(resultsDir, file2)

    DumpfeatureParams(modelParams,pathToFile1)

    DumpfeatureParams(featureParams,pathToFile2)
    

    return pathToSave


def get_metrics(userAnnotatedFile, groundTruthFile, event):

    ins = 0
    dele = 0
    fp = fn = tp = 0.0
    total = 0.0

    with open(userAnnotatedFile) as userfile, open(groundTruthFile) as truthfile: 
        for x, y in zip(userfile, truthfile):

            userline = x.strip().split()
            truthline = y.strip().split()


            if(len(userline) > len(truthline) ):
                fp += 1
                ins += 1
                total+=1

            elif( len(truthline) > len(userline)):
                
                if(len(truthline) == 4):
                    fn+=1
                    dele+=1
                    total += 1

            elif( len(truthline) == len(userline) and len(truthline) == 4):
                total+=1
                #if event within collar
                collar = abs(float(userline[1]) - float(truthline[1]))
                if collar <= 0.5 and userline[3] == truthline[3]:
                    tp += 1


                #not within collar, but an event is present: Clarify this!
                else:
                    if(float(userline[1]) > float(truthline[1])):  #detected after actual event .missed actual event
                        dele+=1
                        fn+=1
                        
                    else:                                         #detected before actual event started
                        fp+=1
                        ins+=1
            else:
                total+=1
                        

    er = (ins+dele)/total
    Fscore = 2*tp/ (2*tp + fp + fn)


    return er, Fscore,fp,fn,ins,dele,tp


#userAnnotatedFile = 'f1.txt'
#groundTruthFile = 'f2.txt'

#evaluate(userAnnotatedFile,groundTruthFile,'babycry')

